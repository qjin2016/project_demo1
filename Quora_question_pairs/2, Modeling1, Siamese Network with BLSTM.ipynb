{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### This notebook is used for modeling with Siamese Network based on Birectional Long Short Term Memory Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import csv, json\n",
    "from zipfile import ZipFile\n",
    "from os.path import expanduser, exists\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers import Input, Dense, Conv1D, Dropout, MaxPooling1D, Flatten, Embedding, LSTM, Bidirectional, merge, dot\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('./data/training_py2.pickle', 'rb') as handle:\n",
    "    question1_word_sequences, question2_word_sequences, y_train = pickle.load(handle)\n",
    "    \n",
    "with open('./data/testing_py2.pickle', 'rb') as handle:\n",
    "    question1_test_ws, question2_test_ws = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load embedding matrix\n",
    "with open('./data/word_embedding_matrix_glove100_py2.pickle', 'rb') as handle:\n",
    "    word_embedding_glove100 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1_data = pad_sequences(question1_word_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(question2_word_sequences, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# left subnet\n",
    "q1 = Input(shape = (MAX_SEQUENCE_LENGTH, ), dtype = 'int32')\n",
    "q1_emb = Embedding(137043, 100,\n",
    "                            weights = [word_embedding_glove100],\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            trainable = False, mask_zero = True)(q1)\n",
    "q1_bi = Bidirectional(LSTM(64))(q1_emb)\n",
    "\n",
    "# right subnet\n",
    "q2 = Input(shape = (MAX_SEQUENCE_LENGTH, ), dtype = 'int32')\n",
    "q2_emb = Embedding(137043, 100,\n",
    "                            weights = [word_embedding_glove100],\n",
    "                            input_length = MAX_SEQUENCE_LENGTH,\n",
    "                            trainable = False, mask_zero = True)(q2)\n",
    "q2_bi = Bidirectional(LSTM(64))(q2_emb)\n",
    "\n",
    "# merge\n",
    "merged = dot([q1_bi, q2_bi], axes = 1, normalize = True)\n",
    "\n",
    "model = Model(inputs = [q1, q2], outputs = merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', \n",
    "              loss = 'binary_crossentropy', metrics=['binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/1000\n",
      "363861/363861 [==============================] - 566s - loss: 0.5202 - binary_crossentropy: 0.5202 - val_loss: 0.4914 - val_binary_crossentropy: 0.4914\n",
      "Epoch 2/1000\n",
      "363861/363861 [==============================] - 567s - loss: 0.4879 - binary_crossentropy: 0.4879 - val_loss: 0.4736 - val_binary_crossentropy: 0.4736\n",
      "Epoch 3/1000\n",
      "363861/363861 [==============================] - 573s - loss: 0.4681 - binary_crossentropy: 0.4681 - val_loss: 0.4556 - val_binary_crossentropy: 0.4556\n",
      "Epoch 4/1000\n",
      "363861/363861 [==============================] - 566s - loss: 0.4515 - binary_crossentropy: 0.4515 - val_loss: 0.4592 - val_binary_crossentropy: 0.4592\n",
      "Epoch 5/1000\n",
      "363861/363861 [==============================] - 587s - loss: 0.4354 - binary_crossentropy: 0.4354 - val_loss: 0.4402 - val_binary_crossentropy: 0.4402\n",
      "Epoch 6/1000\n",
      "363861/363861 [==============================] - 626s - loss: 0.4231 - binary_crossentropy: 0.4231 - val_loss: 0.4433 - val_binary_crossentropy: 0.4433\n",
      "Epoch 7/1000\n",
      "363861/363861 [==============================] - 614s - loss: 0.4114 - binary_crossentropy: 0.4114 - val_loss: 0.4362 - val_binary_crossentropy: 0.4362\n",
      "Epoch 8/1000\n",
      "363861/363861 [==============================] - 592s - loss: 0.4035 - binary_crossentropy: 0.4035 - val_loss: 0.4497 - val_binary_crossentropy: 0.4497\n",
      "Epoch 9/1000\n",
      "363861/363861 [==============================] - 568s - loss: 0.3945 - binary_crossentropy: 0.3945 - val_loss: 0.4299 - val_binary_crossentropy: 0.4299\n",
      "Epoch 10/1000\n",
      "363861/363861 [==============================] - 17292s - loss: 0.3864 - binary_crossentropy: 0.3864 - val_loss: 0.4338 - val_binary_crossentropy: 0.4338\n",
      "Epoch 11/1000\n",
      "363861/363861 [==============================] - 565s - loss: 0.3789 - binary_crossentropy: 0.3789 - val_loss: 0.4310 - val_binary_crossentropy: 0.4310\n",
      "Epoch 12/1000\n",
      "363861/363861 [==============================] - 564s - loss: 0.3722 - binary_crossentropy: 0.3722 - val_loss: 0.4343 - val_binary_crossentropy: 0.4343\n",
      "Epoch 13/1000\n",
      "363861/363861 [==============================] - 564s - loss: 0.3651 - binary_crossentropy: 0.3651 - val_loss: 0.4308 - val_binary_crossentropy: 0.4308\n",
      "Epoch 14/1000\n",
      "363861/363861 [==============================] - 565s - loss: 0.3602 - binary_crossentropy: 0.3602 - val_loss: 0.4479 - val_binary_crossentropy: 0.4479\n",
      "Epoch 15/1000\n",
      "363861/363861 [==============================] - 565s - loss: 0.3547 - binary_crossentropy: 0.3547 - val_loss: 0.4328 - val_binary_crossentropy: 0.4328\n",
      "Epoch 16/1000\n",
      "363861/363861 [==============================] - 565s - loss: 0.3493 - binary_crossentropy: 0.3493 - val_loss: 0.4696 - val_binary_crossentropy: 0.4696\n",
      "Epoch 17/1000\n",
      "363861/363861 [==============================] - 565s - loss: 0.3442 - binary_crossentropy: 0.3442 - val_loss: 0.4311 - val_binary_crossentropy: 0.4311\n",
      "Epoch 18/1000\n",
      "363861/363861 [==============================] - 565s - loss: 0.3395 - binary_crossentropy: 0.3395 - val_loss: 0.4478 - val_binary_crossentropy: 0.4478\n",
      "Epoch 19/1000\n",
      "363861/363861 [==============================] - 566s - loss: 0.3351 - binary_crossentropy: 0.3351 - val_loss: 0.4594 - val_binary_crossentropy: 0.4594\n",
      "Epoch 20/1000\n",
      "363861/363861 [==============================] - 565s - loss: 0.3308 - binary_crossentropy: 0.3308 - val_loss: 0.4548 - val_binary_crossentropy: 0.4548\n",
      "Epoch 00019: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee786940>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = .001, \n",
    "                              patience = 10, verbose = 1, mode = 'auto')\n",
    "\n",
    "model.fit([q1_data, q2_data], y_train, validation_split = .1,\n",
    "          epochs=1000, batch_size=512, verbose = 1, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jqu/miniconda2/envs/py3/lib/python3.5/site-packages/keras/engine/topology.py:2261: UserWarning: Layer bidirectional_1 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'embedding_1/NotEqual:0' shape=(?, 25) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "/Users/jqu/miniconda2/envs/py3/lib/python3.5/site-packages/keras/engine/topology.py:2261: UserWarning: Layer bidirectional_2 was passed non-serializable keyword arguments: {'mask': <tf.Tensor 'embedding_2/NotEqual:0' shape=(?, 25) dtype=bool>}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_train_2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_train_2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "json_file = open('model_train_2.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_train_2.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load id labels\n",
    "test_ids = []\n",
    "# question1_test = []\n",
    "# question2_test = []\n",
    "\n",
    "with open('./data/test.csv', encoding = 'utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter = ',')\n",
    "    for row in reader:\n",
    "        test_ids.append(row['test_id'])\n",
    "#         question1_test.append(row['question1'])\n",
    "#         question2_test.append(row['question2'])\n",
    "# print ('Question pairs in testing dataset: %d' % len(question1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model evaluation on independent testing dataset\n",
    "q1_test_data = pad_sequences(question1_test_ws, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "q2_test_data = pad_sequences(question2_test_ws, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "preds = loaded_model.predict([q1_test_data, q2_test_data])\n",
    "preds_df = pd.DataFrame(test_ids, columns = ['test_id'])\n",
    "preds_df = pd.DataFrame(preds, columns = ['is_duplicate'])\n",
    "preds_df.to_csv('./data/submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
